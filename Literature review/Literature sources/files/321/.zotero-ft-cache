
Skip to main content
Cornell University
We gratefully acknowledge support from
the Simons Foundation and member institutions.
arXiv.org > cs > arXiv:2101.05509

Help | Advanced Search
Search
Computer Science > Computation and Language
(cs)
COVID-19 e-print

Important: e-prints posted on arXiv are not peer-reviewed by arXiv; they should not be relied upon without context to guide clinical practice or health-related behavior and should not be reported in news media as established information without consulting multiple experts in the field.
[Submitted on 14 Jan 2021 ( v1 ), last revised 18 Jan 2021 (this version, v2)]
Title: Transformer-based Language Model Fine-tuning Methods for COVID-19 Fake News Detection
Authors: Ben Chen , Bin Chen , Dehong Gao , Qijin Chen , Chengfu Huo , Xiaonan Meng , Weijun Ren , Yang Zhou
Download PDF

    Abstract: With the pandemic of COVID-19, relevant fake news is spreading all over the sky throughout the social media. Believing in them without discrimination can cause great trouble to people's life. However, universal language models may perform weakly in these fake news detection for lack of large-scale annotated data and sufficient semantic understanding of domain-specific knowledge. While the model trained on corresponding corpora is also mediocre for insufficient learning. In this paper, we propose a novel transformer-based language model fine-tuning approach for these fake news detection. First, the token vocabulary of individual model is expanded for the actual semantics of professional phrases. Second, we adapt the heated-up softmax loss to distinguish the hard-mining samples, which are common for fake news because of the disambiguation of short text. Then, we involve adversarial training to improve the model's robustness. Last, the predicted features extracted by universal language model RoBERTa and domain-specific model CT-BERT are fused by one multiple layer perception to integrate fine-grained and high-level specific representations. Quantitative experimental results evaluated on existing COVID-19 fake news dataset show its superior performances compared to the state-of-the-art methods among various evaluation metrics. Furthermore, the best weighted average F1 score achieves 99.02%. 

Comments: 	9 pages, 1 figures
Subjects: 	Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI)
Cite as: 	arXiv:2101.05509 [cs.CL]
  	(or arXiv:2101.05509v2 [cs.CL] for this version)
Submission history
From: Ben Chen [ view email ]
[v1] Thu, 14 Jan 2021 09:05:42 UTC (146 KB)
[v2] Mon, 18 Jan 2021 15:53:22 UTC (2,005 KB)
Full-text links:
Download:

    PDF
    Other formats 

Current browse context:
cs.CL
< prev   |   next >
new | recent | 2101
Change to browse by:
cs
cs.AI
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

DBLP - CS Bibliography
listing | bibtex
Bin Chen
Xiaonan Meng
Yang Zhou
a export bibtex citation Loading...
Bookmark
BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code & Data
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

