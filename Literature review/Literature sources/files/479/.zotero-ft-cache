
Skip to main content
Cornell University
We gratefully acknowledge support from
the Simons Foundation and member institutions.
arXiv.org > cs > arXiv:1907.11692

Help | Advanced Search
Search
Computer Science > Computation and Language
(cs)
[Submitted on 26 Jul 2019]
Title: RoBERTa: A Robustly Optimized BERT Pretraining Approach
Authors: Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , Veselin Stoyanov
Download PDF

    Abstract: Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 

Subjects: 	Computation and Language (cs.CL)
Cite as: 	arXiv:1907.11692 [cs.CL]
  	(or arXiv:1907.11692v1 [cs.CL] for this version)
Submission history
From: Myle Ott [ view email ]
[v1] Fri, 26 Jul 2019 17:48:29 UTC (45 KB)
Full-text links:
Download:

    PDF
    PostScript
    Other formats 

( license )
Current browse context:
cs.CL
< prev   |   next >
new | recent | 1907
Change to browse by:
cs
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

9 blog links
( what is this? )
DBLP - CS Bibliography
listing | bibtex
Yinhan Liu
Myle Ott
Naman Goyal
Jingfei Du
Mandar Joshi
â€¦
a export bibtex citation Loading...
Bookmark
BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code & Data
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

